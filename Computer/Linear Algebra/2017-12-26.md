## 2017-12-26

- Let $A$ be an $n\times n$ matrix. If there exists a nonzero vector $v$ such that $Av=λv$, then $λ$ is called an __eigenvalue__ of $A$, and the vector $v$ is called the __eigenvector__ of $A$ associated with $λ$.
- $Av=λv$ &rArr; $(A-λI)v=0$. Hence, $v$ is in the nullspace of $A-λI$.
- _Theorem_: $λ$ is an eigenvalue of $A$ &hArr; det($A-λI$) = 0. And for every eigenvalue $λ$, there exists at least one nonzero eigenvector $v$ associated with it.
- Eigenvectors are nonzero vectors and are never unique.
- A $n\times n$ matrix has $n$ (not necessarily) real or complex eigenvalues.
- det($A-λI$) = 0 is called the __characteristic polynomial__ of $A$. For each eigenvalue $λ$, the nullspace of $A-λI$ is called the __eigenspace__ of $A$.
- The eigenvalue is also called the __characteristic value__ or __latent value (root)__. The eigenvector is also called the __characteristic vector__ or __latent vector__.
- A zero eigenvalue means dependent columns and rows and a zero determinant. All invertible matrices have $λ≠0$.
- For a triangular matrix, its eigenvalues are its diagonal elements.
- _Theorem_: Let $A$ be an $n\times n$ matrix w/ $n$ eigenvalues $λ_1,...,λ_n$. Then, trace$A$ = $\sum_i λ_i$ and det$A$ = $\prod_i λ_i$.
- The multiplicity with which an eigenvalue appears is called the __algebraic multiplicity__ of $λ$. The dimension of eigenspace is called the __geometric multiplicity__ of $λ$.
- _Theorem_: algebraic multiplicity ≥ geometric multiplicity.
- A $n\times n$ matrix $A$ is said to be diagonalizable if there exists a nonsingular matrix $S$ such that $S^{-1}AS = \Lambda$.
- _Theorem_: Suppose $A_{n\times n}$ has $n$ linearly independent eigenvectors $x_1,...,x_n$. Let $S$ be an $n\times n$ matrix w/ $x_1,...,x_n$ as columns. Then $S^{-1}AS = \Lambda$ = diag($λ_1,...,λ_n$), where $λ_i$ satisfies $Ax_i = λ_ix_i$.
- If all eigenvalues are different, then all eigenvectors are linearly independent and all geometric and algebraic multiplicities are 1. In other words, a matrix with distinct eigenvalues can be diagonalized.
- Te diagonalizing matrix $S$ is not unique. Repeated eigenvalues leave more freedom. For example, $S^{-1}AS = I$ for any nonsingular $S$.
- $AS = S\Lambda$ holds if and only if the columns of $S$ are eigenvectors of $A$.
- Not all matrices possess $n$ linearly independent eigenvectors, and therefore, not all matrices are diagonalizable.
- Diagonalizability is connected with the eigenvectors. Invertibility is connected with eigenvalues.
- The only connection between diagonalizability and invertibility probably is that "diagonalization can fail only if there are repeated eigenvalues".
- _Theorem_: The eigenvectors corresponding to distinct eigenvalues are linearly independent.
- $Q = I - 2uu^\top$ is called a __Householder transformation__. The Householder transformation is a reflection which reflects about the axis perpendicular to $u$.
